annotate('text',label='Reject Null Above Green Line',color='darkgreen',x=min(n_correlBF_threshs$n)+4,y=.27) +
annotate('text',label='Accept Null Below Blue Line',color='darkblue',x=min(n_correlBF_threshs$n)+4,y=.23) +
labs(y = 'Correlation (rho)'
,x = 'Sample Size'
)
ggplot(n_correlBF_threshs,aes(x = n)) + theme_bw() +
geom_line(aes(x = n, y = r_acceptNull)
, data = aggregate(r_acceptNull ~ n, n_correlBF_threshs,median)
,color = 'darkblue') +
geom_line(aes(x = n, y = r_rejectNull)
, data = aggregate(r_rejectNull ~ n, n_correlBF_threshs,median)
,color = 'darkgreen') +
stat_summary(aes(y = r_rejectNull), geom = 'ribbon',fun.data = mean_cl_boot) +
geom_jitter(aes(y = r_acceptNull),col='darkblue',width = .1) +
geom_jitter(aes(y = r_rejectNull),col='darkgreen',width = .1) +
annotate('text',label='Reject Null Above Green Line',color='darkgreen',x=min(n_correlBF_threshs$n)+4,y=.27) +
annotate('text',label='Accept Null Below Blue Line',color='darkblue',x=min(n_correlBF_threshs$n)+4,y=.23) +
labs(y = 'Correlation (rho)'
,x = 'Sample Size'
)
ggplot(n_correlBF_threshs,aes(x = n)) + theme_bw() +
geom_line(aes(x = n, y = r_acceptNull)
, data = aggregate(r_acceptNull ~ n, n_correlBF_threshs,median)
,color = 'darkblue') +
geom_line(aes(x = n, y = r_rejectNull)
, data = aggregate(r_rejectNull ~ n, n_correlBF_threshs,median)
,color = 'darkgreen') +
stat_summary(aes(y = r_rejectNull), geom = 'ribbon',fun.data = mean_cl_boot,alpha = .5) +
geom_jitter(aes(y = r_acceptNull),col='darkblue',width = .1) +
geom_jitter(aes(y = r_rejectNull),col='darkgreen',width = .1) +
annotate('text',label='Reject Null Above Green Line',color='darkgreen',x=min(n_correlBF_threshs$n)+4,y=.27) +
annotate('text',label='Accept Null Below Blue Line',color='darkblue',x=min(n_correlBF_threshs$n)+4,y=.23) +
labs(y = 'Correlation (rho)'
,x = 'Sample Size'
)
ggplot(n_correlBF_threshs,aes(x = n)) + theme_bw() +
geom_line(aes(x = n, y = r_acceptNull)
, data = aggregate(r_acceptNull ~ n, n_correlBF_threshs,median)
,color = 'darkblue') +
geom_line(aes(x = n, y = r_rejectNull)
, data = aggregate(r_rejectNull ~ n, n_correlBF_threshs,median)
,color = 'darkgreen') +
# stat_summary(aes(y = r_rejectNull), geom = 'ribbon',fun.data = mean_cl_boot,alpha = .5) +
geom_jitter(aes(y = r_acceptNull),col='darkblue',width = .1) +
geom_jitter(aes(y = r_rejectNull),col='darkgreen',width = .1) +
annotate('text',label='Reject Null Above Green Line',color='darkgreen',x=min(n_correlBF_threshs$n)+4,y=.27) +
annotate('text',label='Accept Null Below Blue Line',color='darkblue',x=min(n_correlBF_threshs$n)+4,y=.23) +
labs(y = 'Correlation (rho)'
,x = 'Sample Size'
)
rm(list = ls())
d <- data.frame(x = rep(1:200,3),y = rep(rnorm(3),each = 200), agent = rep(letters[1:3],each=200))
library(ggplot2)
ggplot(d,aes(x=x,y=y))
ggplot(d,aes(x=x,y=y)) +
theme_bw() +
geom_line()
rep(rnorm(3),each = 200)
ggplot(d,aes(x=x,y=y,col=agent)) +
theme_bw() +
geom_line()
ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y))
ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y)) +
labs(x= 'time point'
,y = 'output value')
ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y+rnorm(600,sd=.5))) +
labs(x= 'time point'
,y = 'output value')
ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y)) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value')
ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y+rnorm(600,sd=.5))) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value')
ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y)) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value'
,title = 'Subcritical regime')
ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y+rnorm(600,sd=.5))) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value'
,title = 'Supercritical regime')
ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y+rnorm(600,sd=.1) + sin((1:600/25)))) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value'
,title = 'Critical regime')
ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y+rnorm(600,sd=.1) + sin((1:600/75)))) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value'
,title = 'Critical regime')
ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y+rnorm(600,sd=.1) + sin((1:600/19)))) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value'
,title = 'Critical regime')
library(ggplot2) ; library(gridExtra)
grid.arrange(
ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y)) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value'
,title = 'Subcritical regime')
,ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y+rnorm(600,sd=.5))) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value'
,title = 'Supercritical regime')
,ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y+rnorm(600,sd=.1) + sin((1:600/19)))) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value'
,title = 'Critical regime')
,ncol=3
)
grid.arrange(
ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y)) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value'
,title = 'Subcritical regime')
,ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y+rnorm(600,sd=.1) + sin((1:600/19))/2 )) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value'
,title = 'Critical regime')
,ggplot(d,aes(x=x,col=agent)) +
theme_bw() +
geom_line(aes(y=y+rnorm(600,sd=.5))) +
coord_cartesian(ylim = c(-2,2)) +
labs(x= 'time point'
,y = 'output value'
,title = 'Supercritical regime')
,ncol=3
)
rm(list = ls())
?TEfits::TEbrm
rm(list = ls())
load("I:/My Drive/writing/USN/data_and_fits/model_fits_standard/usn_partial_wNFS_ahve_10tree_wExclusions.RData")
rm(list = ls())
load("I:/My Drive/writing/USN/data_and_fits/model_fits_standard/usn_partial_wNFS_bnjv_10tree_wExclusions.RData")
load("I:/My Drive/writing/USN/data_and_fits/model_fits_standard/usn_partial_wNFS_bnjv_10tree_wExclusions.RData")
load("I:/My Drive/writing/USN/data_and_fits/model_fits_standard/usn_partial_wNFS_cjzi_12tree_wExclusions.RData")
load("I:/My Drive/writing/USN/data_and_fits/model_fits_standard/usn_partial_wNFS_fzts_10tree_wExclusions.RData")
library(gridExtra)
library(brms)
library(ggplot2)
library(knitr)
library(ACmisc)
library(readxl)
library(quantreg)
knitr::opts_chunk$set(echo = TRUE)
sourceIfChanged('prep_usn_fits.R')
library(gridExtra)
library(brms)
library(ggplot2)
library(knitr)
library(ACmisc)
library(readxl)
library(quantreg)
knitr::opts_chunk$set(echo = TRUE)
sourceIfChanged('prep_usn_fits.R')
# # # # # flow between files:
## > fits and such exist in a bunch of different files and folders (within /data_and_fits/)
## > the script `collect_usn_fits.R` grabs them and puts them into a single file (over 250mb)
## > the script `prep_usn_fits.R` trims some of the space-consuming parts
## > > > and it also runs some of the analyses that take a couple minutes, like mediations
## > > > and then it saves a file
## > then this .Rmd file imports the outputs of `prep_usn_fits.R` and presents the results
#>>> HOTAS experience is problematic because of sparseness
#>>> dive into "what subtest (block + subtest) drives the big difference"?? (LASSO with harsh? prob just block 1)
#> is the fact that VTT started everything biasing things? because it's so easy, and
# # # has floor effects, and the curves are sensitive to beginning values?
#
## > include comparisons to smaller block numbers
#
# > get correlation between end-of-6th-block and asymptote!
## > organize around the individual differences
## > > mediations etc
#
# > > check to see if (and identify any) people are missing from the demogs or anything else ("follow up with this person" thing)
# > > > > Nathan says that 14 people are missing currently (missing their gaming survey)
#
## > does AVGP or simulator mitigate differences between NFS and non-NFS?
#
## > show pp checks (if they look helpful)
#
## > include comparisons to medians
#
## > include comparisons to smaller sample sizes
p_fitted_by_start <- ggplot(standardMods$fitMods_fitted
,aes(x = totalDistNum
, y=Estimate_scaled_to_start
, color = starting_tertile)) +
theme_bw() +
facet_grid(~tertiles) +
geom_line(aes(group =  subID)
# ,color = 'black'
, alpha = .2) +
# geom_smooth(aes(color = Phase
#                 # , linetype = subtestSequenceFact
# )
# ,method = 'loess' , span = .4 , method.args = list(degree = 0)
# , se = F ) +
labs(title = paste0(figCounter,'. Overall Tertiles') , x = 'Distance measurement number (overall)' , y = 'Normalized distance to starting point estimate distribution\nParticipant-level fits (colored lines)'
,x = 'Overall time point')
## To fit a standard LMEM augmented with mixed-effect bases
library(ACmisc)
library(TEfits)
library(dplyr)
library(gamm4)
nRunsPer <- 10
genBasisOffsets <- function(trials,interPeakWidth){
dTmp <- time_basisFun_df(trials,interPeakWidth) # create the basis vectors
dTmp <- dTmp[,2:ncol(dTmp)]
basisRandomOffsetN <- ceiling(ncol(dTmp) / 3) # how many bases should be negative and how many positive
if(basisRandomOffsetN > 1){
shuffleCols <- sample(1:ncol(dTmp))
# create the fluctuation vector by summing the weighted bases:
basisRandomOffset <- rowSums(dTmp[,shuffleCols[1:basisRandomOffsetN]]) -
rowSums(dTmp[,shuffleCols[(basisRandomOffsetN+1):(2*basisRandomOffsetN)]])
}else{
# if the number of trials is not enough to a large number of bases, at the peak width:
basisRandomOffset <- dTmp[,1] -
dTmp[,2]
}
return(basisRandomOffset)
}
genDat <- function(nTrials,nSubj,interPeakWidth,cycleBeta,xVarBeta){
do.call('rbind'
,replicate(nSubj,{
subjDat <- data.frame(
subID = paste0(sample(letters,4),sample(0:9,4),collapse='')
,trialNum = 1:nTrials
,xVar = sample(c(-.5,.5),nTrials,replace = T)
)
subjDat$subjBases <- genBasisOffsets(subjDat$trialNum, interPeakWidth + rnorm(1))
subjDat$yVar <- rnorm(nTrials) + # "true" noise with sd 1
xVarBeta * rnorm(1,1,.1) * subjDat$xVar + # "X" with random subject-level offset
cycleBeta * subjDat$subjBases # Bases over time
subjDat
},simplify = F)
)
}
allSimMat <- expand.grid(
trialN = c(seq(150,500,50)) #
,subjN  = seq(20,30,10) #
,peakWidth = c(seq(20,50,5),seq(60,100,10)) #
,basisWidth = c(seq(10,50,5),seq(70,100,10)) #
,fluctBeta = seq(.1,.5,.1)
,xVarBeta = seq(.1,.5,.1)
)
allSimMat <- expand.grid(
trialN = c(seq(150,500,50)) #
,subjN  = seq(20,30,10) #
,peakWidth = c(seq(20,50,5),seq(60,100,10)) #
,basisWidth = c(seq(10,50,5),seq(70,100,10)) #
,fluctBeta = seq(.1,.5,.1)
,xVarBeta = seq(.1,.5,.1)
) ; allSimMat <- allSimMat[allSimMat$trialN/3 > allSimMat$peakWidth,]
rm(list = ls())
load("G:/My Drive/Aaron/misc/basis_sims_e0v1c2h3.RData")
summary(simDF)
d <- simDF ; rm(simDF)
load("G:/My Drive/Aaron/misc/basis_sims_d3y0a1n5.RData")
simDF <- rbind(simDF,d)
aggregate(. ~ basis_beta, simDF, mean)
rm(list = ls())
d_sd1 <- data.frame(y = rnorm(200), x = sample(c('a','b'),200,replace=T))
d_sd1[d_sd1$x == 'b','y'] <- d_sd1[d_sd1$x == 'b','y'] + .5
d_sd5 <- data.frame(y = rnorm(200), x = sample(c('a','b'),200,replace=T))
d_sd5[d_sd5$x == 'b','y'] <- d_sd5[d_sd5$x == 'b','y'] + .5
lm(y~x,d_sd1)
lm(y~x,d_sd5)
summary(lm(y~x,d_sd5))
stdcoeff <- function (MOD)  {b <- summary(MOD)$coef[-1, 1] ;    sx <- sd(MOD$model[-1]);     sy <- sd(MOD$model[1]);     beta <- b * sx/sy   ;  return(beta) }
stdcoeff( lm(y~x,d_sd5) )
dd <- lm(y~x,d_sd5)
dd$model[-1]
> d_sd1 <- data.frame(y = rnorm(200), x = sample(c(0,1),200,replace=T))
> d_sd1[d_sd1$x == 1,'y'] <- d_sd1[d_sd1$x == 'b','y'] + .5
stdcoeff <- function (MOD)  {b <- summary(MOD)$coef[-1, 1] ;    sx <- sd(MOD$model[-1]);     sy <- sd(MOD$model[1]);     beta <- b * sx/sy   ;  return(beta) }
stdcoeff(lm(y~x,d_sd1))
d_sd1 <- data.frame(y = rnorm(200), x = sample(c(0,1),200,replace=T))
d_sd1[d_sd1$x == 1,'y'] <- d_sd1[d_sd1$x == 'b','y'] + .5
d_sd5 <- data.frame(y = rnorm(200), x = sample(c(0,1),200,replace=T))
d_sd5[d_sd5$x == 1,'y'] <- d_sd5[d_sd5$x == 1,'y'] + .5
stdcoeff <- function (MOD)  {b <- summary(MOD)$coef[-1, 1] ;    sx <- sd(MOD$model[-1]);     sy <- sd(MOD$model[1]);     beta <- b * sx/sy   ;  return(beta) }
stdcoeff(lm(y~x,d_sd1))
d_sd1 <- data.frame(y = rnorm(200), x = sample(c(0,1),200,replace=T))
d_sd1[d_sd1$x == 1,'y'] <- d_sd1[d_sd1$x == 1,'y'] + .5
d_sd5 <- data.frame(y = rnorm(200), x = sample(c(0,1),200,replace=T))
d_sd5[d_sd5$x == 1,'y'] <- d_sd5[d_sd5$x == 1,'y'] + .5
stdcoeff <- function (MOD)  {b <- summary(MOD)$coef[-1, 1] ;    sx <- sd(MOD$model[-1]);     sy <- sd(MOD$model[1]);     beta <- b * sx/sy   ;  return(beta) }
stdcoeff(lm(y~x,d_sd1))
install.packages('QuantPsyc')
rm(list= ls())
library(QuantPsyc)
?lm.beta
d_sd1 <- data.frame(y = rnorm(200), x = sample(c('a','b'),200,replace=T))
d_sd1[d_sd1$x == 'b','y'] <- d_sd1[d_sd1$x == 'b','y'] + .5
d_sd5 <- data.frame(y = rnorm(200), x = sample(c('a','b'),200,replace=T))
d_sd5[d_sd5$x == 'b','y'] <- d_sd5[d_sd5$x == 'b','y'] + .5
library(QuantPsyc)
lm.beta(lm(y~x,d_sd1))
d_sd1 <- data.frame(y = rnorm(200), x = sample(c(0,1),200,replace=T))
d_sd1 <- data.frame(y = rnorm(200), x = sample(c(0,1),200,replace=T))
d_sd1$y <- d_sd1$y + .25 * d_sd1$x
lm.beta(lm(y~x,d_sd1))
d_sd1 <- data.frame(y = rnorm(200), x = sample(c(0,1),200,replace=T))
d_sd1$y <- d_sd1$y + .25 * d_sd1$x
d_sd5 <- data.frame(y = rnorm(200,sd = 5), x = sample(c(0,1),200,replace=T))
d_sd5$y <- d_sd5$y + .25 * d_sd5$x
library(QuantPsyc)
lm.beta(lm(y~x,d_sd1))
nObs <- 1E3
condDiff <- .25
d_sd1 <- data.frame(y = rnorm(nObs), x = sample(c(0,1),nObs,replace=T))
d_sd1$y <- d_sd1$y + .condDiff * d_sd1$x
d_sd5 <- data.frame(y = rnorm(nObs,sd = 5), x = sample(c(0,1),nObs,replace=T))
d_sd5$y <- d_sd5$y + condDiff * d_sd5$x
library(QuantPsyc)
summary(lm(y~x,d_sd1))
lm.beta(lm(y~x,d_sd1))
summary(lm(y~x,d_sd5))
lm.beta(lm(y~x,d_sd5))
nObs <- 1E3
condDiff <- .25
d_sd1 <- data.frame(y = rnorm(nObs), x = sample(c(0,1),nObs,replace=T))
d_sd1$y <- d_sd1$y + .condDiff * d_sd1$x
d_sd5 <- data.frame(y = rnorm(nObs,sd = 5), x = sample(c(0,1),nObs,replace=T))
d_sd5$y <- d_sd5$y + condDiff * d_sd5$x
library(QuantPsyc)
summary(lm(y~x,d_sd1))
lm.beta(lm(y~x,d_sd1))
summary(lm(y~x,d_sd5))
lm.beta(lm(y~x,d_sd5))
nObs <- 1E4
condDiff <- .25
d_sd1 <- data.frame(y = rnorm(nObs), x = sample(c(0,1),nObs,replace=T))
d_sd1$y <- d_sd1$y + .condDiff * d_sd1$x
d_sd5 <- data.frame(y = rnorm(nObs,sd = 5), x = sample(c(0,1),nObs,replace=T))
d_sd5$y <- d_sd5$y + condDiff * d_sd5$x
library(QuantPsyc)
summary(lm(y~x,d_sd1))
lm.beta(lm(y~x,d_sd1))
summary(lm(y~x,d_sd5))
lm.beta(lm(y~x,d_sd5))
mean(d_sd1[d_sd1$x == 0,'y'])
d_sd1 <- data.frame(y = rnorm(nObs), x = sample(c(0,1),nObs,replace=T))
d_sd1$y <- d_sd1$y + condDiff * d_sd1$x
d_sd5 <- data.frame(y = rnorm(nObs,sd = 5), x = sample(c(0,1),nObs,replace=T))
d_sd5$y <- d_sd5$y + condDiff * d_sd5$x
library(QuantPsyc)
summary(lm(y~x,d_sd1))
lm.beta(lm(y~x,d_sd1))
summary(lm(y~x,d_sd5))
lm.beta(lm(y~x,d_sd5))
nObs <- 1E4
condDiff <- .25
d_sd1 <- data.frame(y = rnorm(nObs), x = sample(c(0,1),nObs,replace=T))
d_sd1$y <- d_sd1$y + condDiff * d_sd1$x
d_sd5 <- data.frame(y = rnorm(nObs,sd = 5), x = sample(c(0,1),nObs,replace=T))
d_sd5$y <- d_sd5$y + condDiff * d_sd5$x
library(QuantPsyc)
summary(lm(y~x,d_sd1))
lm.beta(lm(y~x,d_sd1))
summary(lm(y~x,d_sd5))
lm.beta(lm(y~x,d_sd5))
nObs <- 1E4
condDiff <- .25
d_sd1 <- data.frame(y = rnorm(nObs), x = sample(c(0,1),nObs,replace=T))
d_sd1$y <- d_sd1$y + condDiff * d_sd1$x
d_sd5 <- data.frame(y = rnorm(nObs,sd = 5), x = sample(c(0,1),nObs,replace=T))
d_sd5$y <- d_sd5$y + condDiff * d_sd5$x
library(QuantPsyc)
summary(lm(y~x,d_sd1))
summary(lm(scale(y)~scale(x),d_sd1))
summary(lm(y~x,d_sd5))
summary(lm(scale(y)~scale(x),d_sd5))
rm(list = ls())
setwd('I:/my drive/basisFunctionPaper/lmem_sim_results/')
source('collect_lmem_sims.R')
pList$allSims_power_peakProp <- polishPlots(ggplot(simDat,aes(x = basis_beta
,y = basis_delta_xVar_t
, linetype = improved_oos
,color = match_basis_peak , fill = match_basis_peak
)) +
geom_abline(linetype = 3, color = 'black',slope = 0) +
stat_summary(fun.data = median_hilow,fun.args=list(conf.int=confintSize), geom = 'ribbon',alpha = .2,color = rgb(0,0,0,0)) +
stat_summary(fun = mean,geom = 'line',linewidth=1) +
# coord_cartesian(ylim = c(-.05,.25)) + # need to update beyond 4.1 for this to work.
labs(y = 'Change in fixed effect T value\nwhen including basis functions'
,linetype = 'Model basis width vs\ntrue fluctuation width'
,color = 'Out-of-sample delta-LL')
)
pList$allSims_power_peakProp
rm(list = ls())
source('collect_lmem_sims.R')
min(simDat[simDat$basis_beta == .3, 'basis_delta_logLik_outOfSample'])
simDat[simDat$basis_beta == .3, 'basis_delta_logLik_outOfSample']
simDat[simDat$basis_beta == .3, 'basis_delta_logLik_outOfSample']
simDat[, 'basis_delta_logLik_outOfSample']
simDat[simDat$basis_beta == .3,]
unique(simDat$basis_beta)
simDat[simDat$basis_beta == 0.3,]
simDat[simDat$basis_beta == 0.30,]
simDat$basis_beta
simDat$basis_beta == .3
simDat$basis_beta*10 == 3
simDat$basis_beta*10
simDat$basis_beta == median(simDat$basis_beta)
median(simDat$basis_beta)
simDat$basis_beta == 0.3
as.numeric(simDat$basis_beta) == 0.3
simDat$basis_beta == as.numeric(0.3)
rm(list = ls())
source('collect_lmem_sims.R')
View(TEfits::time_basisFun_df())
View(TEfits::time_basisFun_df)
rm(list = ls())
timeVar <- 1:200
basisDens <- 20
constAdj <- sqrt(2 * log(2)) /2
calc_fun <- switch(basis_calc_fun
,gaussian = function(...,width){dnorm(...,sd=width/constAdj )}
)
basis_calc_fun='gaussian'
constAdj <- sqrt(2 * log(2)) /2
calc_fun <- switch(basis_calc_fun
,gaussian = function(...,width){dnorm(...,sd=width/constAdj )}
)
basisDF_ref <- data.frame(time_orig = sort(unique(na.omit(timeVar))))
basisCenters <- seq(min(basisDF_ref$time_orig) - (basisDens+1)
,max(basisDF_ref$time_orig) + basisDens
,basisDens)
basisCenters <- seq(min(basisDF_ref$time_orig) - (basisDens/2+1)
,max(basisDF_ref$time_orig) + basisDens/2
,basisDens)
basisNamePrec <- 3 - nchar(signif(max(basisCenters) - min(basisCenters),3))
for(curBasisCenter in basisCenters){
basisDF_ref[,paste0('basis_cen_',round(curBasisCenter,basisNamePrec))] <-
calc_fun(basisDF_ref$time_orig, curBasisCenter, width = basisDens/2)
}
colnames(basisDF_ref) <- gsub('-','M', colnames(basisDF_ref),fixed = T)
basisDF_ref[,2:ncol(basisDF_ref)] <-
basisDF_ref[,2:ncol(basisDF_ref)] / rowSums(basisDF_ref[,2:ncol(basisDF_ref)])
timeOut <- data.frame(order_orig = 1:length(timeVar),time_orig = timeVar)
timeOut <- merge(timeOut,  basisDF_ref, by='time_orig')
timeOut <- timeOut[order(timeOut$order_orig),]
timeOut$order_orig <- NULL
attr(timeOut,'basis_centers') <- basisCenters
basisSum <- apply(timeOut[,2:13],1,sum)
plot(basisSum)
plot(timeOut[,1],timeOut[,2])
plot(timeOut[,1],timeOut[,5])
plot(timeOut[,1],timeOut[,5],'l')
for(curCol in 2:13){lines(timeOut[,1],timeOut[,curCol],col=rgb(curCol/20,1-curCol/20,.5))}
mean(diff(timeVar))
signif(mean(diff(timeVar)),2)
signif(mean(diff(timeVar)),2)
lowerOffset <- signif(mean(diff(timeVar)),2)
timeVar <- seq(.001,.6,.02)
lowerOffset <- signif(mean(diff(timeVar)),2)
basisCenters <- seq(min(basisDF_ref$time_orig) - (basisDens/2+lowerOffset)
,max(basisDF_ref$time_orig) + basisDens/2
,basisDens)
basisDF_ref <- data.frame(time_orig = sort(unique(na.omit(timeVar))))
basisCenters <- seq(min(basisDF_ref$time_orig) - (basisDens/2+lowerOffset)
,max(basisDF_ref$time_orig) + basisDens/2
,basisDens)
rm(list = ls())
setwd('c:/Users/coch0/Documents/GitHub/TEfits/')
library(roxygen2)
library(devtools)
document()
install()
