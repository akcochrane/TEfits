library(projpred)
library(bayesplot)
#' see:
#' https://avehtari.github.io/modelselection/bodyfat.html
#' https://mc-stan.org/projpred/articles/projpred.html#modtypes
#' https://mc-stan.org/projpred/reference/cv_varsel.html
d <- AaronFuns::dat_ac_fyp
m <- brm(formula(paste('ravens ~'
,paste(colnames(d)[c(3:9,11:16)]
,'*isAdult'
,sep=''
,collapse='+'
)
)
)
,data = d[d$goodRavens == T,]
# ,prior = prior(horseshoe(),class = 'b') # if regularization was desired
)
## this is the "workhorse" function, but this is not quick to do
## could easily similarly do kfold with a large k, I think
m_cvvs <- cv_varsel(m
, method = 'backward'
# , method = 'forward'
, cv_method = 'LOO',
)
## this is the "workhorse" function, but this is not quick to do
## could easily similarly do kfold with a large k, I think
m_cvvs <- cv_varsel(m
, method = 'forward'
, cv_method = 'LOO',
)
m_cvvs_k20 <- cv_varsel(m
, method = 'forward'
, cv_method = 'kfold'
,K = 20
)
plot(m_cvvs, stats = c('elpd', 'rmse'), deltas=FALSE)
nsel <- suggest_size(m_cvvs)
nsel <- suggest_size(m_cvvs)
plot(m_cvvs_k20, stats = c('elpd', 'rmse'), deltas=FALSE)
suggest_size(m_cvvs_k20)
?suggest_size
summary(m_cvvs)
nsel <- suggest_size(m_cvvs,alpha = .2)
nsel <- suggest_size(m_cvvs,alpha = .9)
nsel <- suggest_size(m_cvvs)
vsel <- solution_terms(m_cvvs)[1:nsel]
qnorm(.8) - qnorm(.2)
qnorm(.85) - qnorm(.15)
library(TEfits)
d <- anstrain
?TEbrm
m11 <- TEbrm(acc ~ tef_change_gam('trialNum', groupingVar = 'subID')
,data = anstrain)
m11
conditional_effects(m11)
m_gam_logis <-
TEbrm(acc ~ tef_change_gam('trialNum', groupingVar = 'subID')
,family = bernoulli()
,data = anstrain)
m_gam_logis
conditional_effects(m_gam_logis)
conditional_effects(m11)
rm(list= ls())
rm(list= ls())
rm(list= ls())
knitr::opts_chunk$set(echo = F)
rmdDir <- getwd()
source('get_nlm_sim_summs.R') ; dFull_nlm <- dFull ; rm(dFull)
source('get_gnlm_sim_summs.R') ; dFull_gnlm <- dFull ; rm(dFull)
knitr::opts_chunk$set(echo = F)
rmdDir <- getwd()
source('get_nlm_sim_summs.R') ; dFull_nlm <- dFull ; rm(dFull)
source('get_gnlm_sim_summs.R') ; dFull_gnlm <- dFull ; rm(dFull)
source('get_nlmem_sim_summs.R') ; dFull_nlmem <- dFull ; rm(dFull)
timeConstBounds <- sort(c(
quantile(2^d_gnlm$pRate_Intercept,c(.02,.98))
,quantile(2^d_nlm$pRate_Intercept,c(.02,.98))
,quantile(2^d_nlmem$pRate_Intercept,c(.02,.98))
))[3:4]
timeConstSplits <- round(seq(3,timeConstBounds[2],length = 31))
sampleNBounds <- sort(c(
quantile(d_gnlm$sample_n,c(.01,.98))
,quantile(d_nlm$sample_n,c(.01,.98))
,quantile(d_nlmem$sample_n,c(.01,.98))
))[3:4]
sampleNSplits <- round(seq(sampleNBounds[1],min(sampleNBounds[2],max(timeConstSplits)*2),length = 11))
samplingDist <- function(x,nresamples = 1E4){
sampleDist <- replicate(1E4,{median(sample(x,replace=T))})
dOut <- ecdf(sampleDist)
attr(dOut,'sampleDist') <- sampleDist
dOut
}
varDist <- function(var_, string = T,digits = 2){
var_dist <- quantile(attr(samplingDist(var_),'sampleDist'),c(.025,.5,.975))
if(string){
return(paste0('*b* = ',round(var_dist['50%'],digits) , '; CI~95~ = [',round(var_dist['2.5%'],digits) , ',', round(var_dist['97.5%'],digits) , ']'))
}
}
d_gnlm$timeToHalfChange <- 2^d_gnlm$pRate_Intercept
modRHS <- c('sample_n' , 'changeAbsD', 'timeToHalfChange')
newDat_gnlm <- data.frame(expand.grid(sample_n = sampleNSplits
,partic_n = round(seq(min(d_gnlm$partic_n),max(d_gnlm$partic_n),length = 5))
,changeAbsD = seq(min(d_gnlm$changeAbsD),10,length = 31)
,timeToHalfChange = timeConstSplits
))
d_gnlm[,paste0(modRHS,'_log')] <- log(d_gnlm[,modRHS])
newDat_gnlm[,paste0(modRHS,'_log')] <- log(newDat_gnlm[,modRHS])
dTmp <- d_gnlm
loess_gnlm <- list()
rasters_gnlm <- list()
for(lhs in c('fixefAbsD')){
if(lhs == 'fixefAbsD'){
plotBreaks = c(seq(0,.5,.1),seq(.75,2,.25),4,10)
fillLab <- 'Normalized\nestimation error (d)'
}else{
plotBreaks = c(0,seq(.5,1,.1))
fillLab <- 'Correlation between\ntrue and estimated\nparticipant coefficients'
}
for(fitMethod in c('boot','approx','bayes')){
if(lhs == 'fixefAbsD'){
loess_gnlm[[lhs]][[fitMethod]] <- loess(formula(paste0('log(m_',fitMethod,'_',lhs,') ~ ',paste(paste0(modRHS,'_log'),collapse='+'))), dTmp)
newDat_gnlm[,paste0(fitMethod,'_',lhs)] <- exp(predict(loess_gnlm[[lhs]][[fitMethod]],newdata = data.frame(newDat_gnlm)))
}
newDat_gnlm$curLHS <- newDat_gnlm[,paste0(fitMethod,'_',lhs)]
#
rasters_gnlm[[lhs]][[fitMethod]] <-
ggplot(newDat_gnlm[
newDat_gnlm$sample_n %in% quantile(newDat_gnlm$sample_n,c(0,.33,.66,1))
,]
,aes( x = changeAbsD
, y = timeToHalfChange
, z = curLHS)
) + theme_bw() + theme(text = element_text(family = 'serif')) +
geom_contour_filled(breaks = plotBreaks) +
geom_line(aes(y = sample_n/2),linetype = 2, color = 'darkgrey') +
facet_grid( ~ sample_n) +
scale_y_continuous(trans='log2') +
scale_x_continuous(trans = 'log2') +
labs(#fill = lhs
title = fitMethod
,x = 'Normalized difference (d) between start and asymptote'
,fill = fillLab
,z = fillLab)
}
}; rm(dTmp)
rasters_gnlm$fixefAbsD$bayes
rasters_gnlm$fixefAbsD$boot
rasters_gnlm$fixefAbsD$approx
rasters_gnlm$fixefAbsD$approx
load("G:/My Drive/Aaron/JF/m_jf_exG_impute_delt6.RData")
modList_exg$DR3_BS3_95delt_6kits_ctr
modList_exg$DR3_BS3_95delt_6kits_fdcx
rm(list = ls())
library(ACmisc)
m <- robustLM_bayes(ravens ~ atComp * wmComp + (1 |isAdult) , dat_cochraneEtAl_2019_PLOSOne$)
m <- robustLM_bayes(ravens ~ atComp * wmComp + (1 |isAdult) , dat_cochraneEtAl_2019_PLOSOne)
m
m$bayes_factors
m$dropOneMods$isAdult
m$dropOneMods$wmComp
rm(list = ls())
load("G:/My Drive/Emma/Perceptual Learning - Tamaki et al 2020 Data/Data Files/Exp 2 Transformed Data/model_continuos_8000.RData")
library(brms)
mod_TEbrm
mod_TEbrm_updated
conditional_effects(mod_TEbrm_updated)
xtabs(~condition_reward_c + condition_session_c1 + condition_session_c2 + condition_sleep_c , mod_TEbrm_updated$data)
xtabs(~condition_reward_c + condition_session_c + condition_sleep_c , mod_TEbrm_updated$data)
xtabs(~condition_reward_c + session_c1 + session_c2 + condition_sleep_c , mod_TEbrm_updated$data)
mod_TEbrm_updated$formula
mod_TEbrm_updated$prior
mod_TEbrm$prior
rm(list = ls())
load("G:/My Drive/Emma/Perceptual Learning - Tamaki et al 2020 Data/Data Files/Exp 1 Transformed Data/m_tamaki_exp1_3models_update_2_23.RData")
mod_TEbrm_updated
mod_TEbrm_updated$prior
mod_TEbrm_updated$bayes_mod_TEbrm_alt
mod_TEbrm_updated$fit@sim
mod_TEbrm_updated$fit@date
mod_TEbrm
mod_TEbrm$prior
rm(list = ls())
load("G:/My Drive/Aaron/misc/smaller_usn_rate_prior/usn_partial_wNFS_lpks_11tree_30subsper_withComplex_wLOO.RData")
modList$init_exp3_taskDiffOffset$u7ha6tp8vu7b$criteria
modList$init_exp3_taskDiffOffset_TEsigma_dayChange$u7ha6tp8vu7b$criteria
rm(list = ls())
load("G:/My Drive/Aaron/misc/d_USN_long_2023-04-01.RData")
hist(log(d_long$VTTDistances))
hist(log(d_long$VTT+.1))
hist(log(d_long$VTT+.01))
hist(log(d_long$VTT+1))
hist(log(d_long$VTT+4))
hist(log(d_long$VTT+40))
hist(log(d_long$VTT_2+1))
hist(log(d_long$VTT_4+1))
hist(log(d_long$ATT_4+1))
hist(log(d_long$ATT_4+.1))
hist(log(d_long$ATT_4+1))
hist(log(d_long$VTT_4+1))
rm(list = ls())
library(gridExtra)
library(brms)
library(ggplot2)
library(knitr)
library(ACmisc)
knitr::opts_chunk$set(echo = TRUE)
sourceIfChanged('collect_usn_fits.R')
load('organizational_scripts_and_intermediate_data/d_USN_long_2023-04-01.RData') ; rm(mList)
d_raw_demogs$subID <- paste0('s',d_raw_demogs$subID)
figCounter <- 0
d_overallMeans <- aggregate(Tracking_mean_sqrt_z ~ subID , d_long_downsampled,mean)
colnames(d_overallMeans) <- c('subID', 'meanDist')
d_overallMeans$tertiles <- NA
d_overallMeans[d_overallMeans$meanDist > quantile(d_overallMeans$meanDist,.666),'tertiles'] <- 'Poor Performers'
d_overallMeans[d_overallMeans$meanDist < quantile(d_overallMeans$meanDist,.334),'tertiles'] <- 'High Performers'
d_overallMeans[d_overallMeans$meanDist < quantile(d_overallMeans$meanDist,.667) &
d_overallMeans$meanDist > quantile(d_overallMeans$meanDist,.333)
,'tertiles'] <- 'Mid Performers'
d_long_downsampled <- merge(d_long_downsampled, d_overallMeans, by = 'subID')
d_taskMeans <- aggregate(Tracking_mean_sqrt_z ~ Phase , d_long_downsampled,mean)
colnames(d_taskMeans) <- c('Phase', 'taskMeanDist')
d_long_downsampled <- merge(d_long_downsampled, d_taskMeans, by = 'Phase')
d_long_downsampled$Tracking_mean_sqrt_z_removedTaskMeans <-
d_long_downsampled$Tracking_mean_sqrt_z -
d_long_downsampled$taskMeanDist
goldStand_fitted <- merge(goldStand_fitted, d_overallMeans, by = 'subID')
# TO DO:
## > organize around the individual differences
## > > mediations etc
#
## > show pp checks (if they look helpful)
#
## > include comparisons to medians
#
## > include comparisons to smaller sample sizes
#
## > include comparisons to smaller block numbers
#
## > determine why the nrow(d_subj) != length(unique(goldStandCoef$subID))
figCounter <- figCounter+1
ggplot(goldStand_fitted
,aes(x = totalDistNum, Estimate[,'Mean'])) +
theme_bw() +
facet_grid(~tertiles) +
geom_line(aes(group =  subID)
,color = 'black', alpha = .2) +
geom_smooth(aes(color = Phase
# , linetype = subtestSequenceFact
)
,method = 'loess' , span = .4 , method.args = list(degree = 0)
, se = F ) +
labs(title = paste0(figCounter,'.') , x = 'Distance measurement number (overall)' , y = 'Normalized distance (colored lines)\nParticipant-level fits (black lines)'
,x = 'Overall time point')
figCounter <- figCounter+1
ggplot(goldStand_fitted_allRanefs
,aes(x = totalDistNum, Estimate[,'Mean'])) +
theme_bw() +
facet_grid(~tertiles) +
geom_line(aes(group =  subID)
,color = 'black', alpha = .2) +
geom_smooth(aes(color = Phase
# , linetype = subtestSequenceFact
)
,method = 'loess' , span = .4 , method.args = list(degree = 0)
, se = F ) +
labs(title = paste0(figCounter,'.') , x = 'Distance measurement number (overall)' , y = 'Normalized distance (colored lines)\nParticipant-level fits (black lines)'
,x = 'Overall time point')
figCounter <- figCounter+1
ggplot(goldStand_fitted_allRanefs
,aes(x = totalDistNum, Estimate[,'Mean'])) +
theme_bw() +
geom_line(aes(group =  subID)
,color = 'black', alpha = .2) +
geom_smooth(aes(color = Phase
# , linetype = subtestSequenceFact
)
,method = 'loess' , span = .4 , method.args = list(degree = 0)
, se = F ) +
labs(title = paste0(figCounter,'.') , x = 'Distance measurement number (overall)' , y = 'Normalized distance (colored lines)\nParticipant-level fits (black lines)'
,x = 'Overall time point')
rm(lis t= ls())
rm(list= ls())
27/33
33/27
.22222 * 60
library(brms) ; library(RWiener)
d <- data.frame(
subID = rep(LETTERS[1:5],each = 50)
,prepost = rep(rep(c(0,1),each=25),5)
)
d
.7 + d$prepost/10
d$acc <- rbinom(nrow(d) , size = 1, .7 + d$prepost/10)
d$rt <- .2 + rnorm(nrow(d),0,.1) + rexp(nrow(d),.1)
d$rt <- .2 + rnorm(nrow(d),0,.1) + rexp(nrow(d),.1) ; hist(d$rt , breaks = 50)
d$rt <- .2 + rnorm(nrow(d),0,.1) + rexp(nrow(d),10) ; hist(d$rt , breaks = 50)
d$rt <- .2 + rnorm(nrow(d),0,.05) + rexp(nrow(d),10) ; hist(d$rt , breaks = 50)
d$rt <-  rnorm(nrow(d),.25,.05) + rexp(nrow(d),10) ; hist(d$rt , breaks = 50)
d$rt <-  rnorm(nrow(d),.25,.05) + rexp(nrow(d),15) ; hist(d$rt , breaks = 50)
d$acc <- rbinom(nrow(d) , size = 1, .7 + d$prepost/10)
d$rt <- d$rt <-  rnorm(nrow(d),.35 - d$prepost/10,.05) + rexp(nrow(d),15) ; hist(d$rt , breaks = 50)
d$rt <- d$rt <-  rnorm(nrow(d),.35 - d$prepost/10 + d$acc/10,.05) + rexp(nrow(d),15) ; hist(d$rt , breaks = 50)
d$rt <- d$rt <-  rnorm(nrow(d),.35 - d$prepost/10 + d$acc/10,.05) + rexp(nrow(d),5) ; hist(d$rt , breaks = 50)
aggregate(rt ~ acc + prepost, mean\ )
aggregate(rt ~ acc + prepost, mean )
aggregate(rt ~ acc + prepost,d, mean )
aggregate( acc ~ prepost,d, mean )
aggregate(rt ~ acc + prepost,d, mean )
aggregate(acc ~ prepost,d, mean )
m <- brm(brmsformula(rt | dec(acc) ~ prepost + (prepost|subID)
,bs ~ prepost + (prepost|subID)
,ndt ~ (1|subID)
)
,d
,family = wiener()
)
m <- brm(brmsformula(rt | dec(acc) ~ prepost + (prepost|subCorr|subID)
,bs ~ prepost + (prepost|subCorr|subID)
,ndt ~ (1|subID)
)
,d
,family = wiener()
)
rm(list = ls())
m <- brm(brmsformula(rt | dec(acc) ~ prepost + (prepost|subCorr|subID)
,bs ~ prepost + (prepost|subCorr|subID)
,ndt ~ (1|subID)
)
,d
,family = wiener()
,control = list(adapt_delta = .2)
)
load("G:/My Drive/Aaron/JF/m_jf_jan23mods_all.RData")
mList$DR8_BS16$fitted <- list()
mList$DR8_BS16$fitted$dr <- fitted(mList$DR8_BS16
,scale = 'linear'
, ndraws = 250)
mList$DR8_BS16$fitted$bs <- fitted(mList$DR8_BS16
,dpar = 'bs'
, ndraws = 250)
mList$DR8_BS16$fitted$signed_rt <- predict(mList$DR8_BS16
,negative_rt = T
,summary = F
, ndraws = 25)
dim(mList$DR8_BS16$fitted$signed_rt)
length(apply(abs(mList$DR8_BS16$fitted$signed_rt),1
,mean,trim = .25))
mList$DR8_BS16$fitted$rt <- apply(abs(mList$DR8_BS16$fitted$signed_rt),2
,mean,trim = .25)
mList$DR8_BS16$fitted$accuracy <- apply(mList$DR8_BS16$fitted$signed_rt,2
,function(x){mean(x > 0)})
hist(mList$DR8_BS16$fitted$rt,breaks = 100)
hist(mList$DR8_BS16$fitted$accuracy,breaks = 100)
save(mList,"G:/My Drive/Aaron/JF/m_jf_jan23mods_all_wfitted.RData")
save(mList,file = "G:/My Drive/Aaron/JF/m_jf_jan23mods_all_wfitted.RData")
cor(mList$DR8_BS16$fitted$accuracy,mList$DR8_BS16$data$accuracy)
cor(mList$DR8_BS16$fitted$rt,mList$DR8_BS16$data$RT)
rm(list = ls())
?combn
combn(6,3)
library(brm)
library(brms)
hist(rskew_normal(1E3,0,1,3),breaks = 50)
hist(rskew_normal(1E4,0,1,3),breaks = 50)
hist(rskew_normal(5E4,0,1,3),breaks = 100)
hist(rskew_normal(5E4,0,1,13),breaks = 100)
rskew_normal(5E4,0,1,13) -> d
mean(d)
skew(d)
library(psych)
skew(d)
?rskew_normal
?brmsfamily
skew(rnorm(1000))
skew(rnorm(10000))
rskew_normal(5E5,0,1,13) -> d
mean(d)
skew(d)
1/skew(d)
?softplus
load("G:/My Drive/Aaron/JF/m_jf_logitNDT_trialBasis_all2sess_fixBias_allPriors_xxeck_wLOO_iteratePredicted.RData")
mList$DR16_BS16
x <- 1:100
y <- x/10
plot(x,y)
y <- (x-50)/10
plot(x,y)
plot(x,plogis(y))
rm(list = ls())
setwd('c:/users/coch0/OneDrive - unige.ch/Leitzke_2020/EIC Analysis/')
library(simr) ; library(lme4)
powerSimList <- list()
lme4mods <- list()
ACmisc::resetSeed()
load('lme4mods_forPower_20230601.RData')
n_sims <- 10
allSims <- expand.grid(
effSizes = seq(.05,.75,.05)
,exper = c('m_c','m_e')
,numFac = c('num','fac')
)
for(curRep in 1:10){
ACmisc::resetSeed()
shuffleSimCombs <- allSims[sample(nrow(allSims)),]
runID <- paste0(sample(letters,4),sample(0:9,4),collapse = '')
for(curSim in 1:nrow(shuffleSimCombs)){
curMod <- as.character( shuffleSimCombs[curSim,'exper'] )
curNumFac <- as.character( shuffleSimCombs[curSim,'numFac'] )
curEffSize <- as.numeric( shuffleSimCombs[curSim,'effSizes'] )
effList <- names(fixef(lme4mods[[curMod]][[curNumFac]]))
effList <- effList[effList != '(Intercept)']
effList <- sample(effList)
for(curEff in effList){
cat('. ') ; ACmisc::resetSeed() ; gc() ; Sys.sleep(1) ; gc() ;Sys.sleep(1)
## all effects together
mTmp <- lme4mods[[curMod]][[curNumFac]]
nFixed <- length(fixef(mTmp))
fixef(mTmp)[2:nFixed] <- curEffSize*sign(fixef(mTmp)[2:nFixed])
powerSimList[[curMod]][[curNumFac]][[curEff]]$allEffs[[as.character(curEffSize)]] <-
powerSim(mTmp
,fixed(curEff)
,nsim =  n_sims)
save(powerSimList , lme4mods
,file = paste0('leitzke_sims_',runID,'.RData'))
cat('. ') ; ACmisc::resetSeed() ; gc() ; Sys.sleep(1) ; gc() ;Sys.sleep(1)
## one at a time
mTmp <- lme4mods[[curMod]][[curNumFac]]
fixef(mTmp)[curEff] <- curEffSize
powerSimList[[curMod]][[curNumFac]][[curEff]]$oneEff[[as.character(curEffSize)]] <-
powerSim(mTmp
,fixed(curEff)
,nsim =  n_sims)
save(powerSimList , lme4mods
,file = paste0('leitzke_sims_',runID,'.RData'))
}
}
}
rm(list = ls())
setwd('g:/my drive/aaron/misc')
source('collect_basis_fun_sims.R')
oosPlots$lloos_prop_by_prop
oosPlots$lloos_prop_by_prop_smooth
oosPlots$deltaT_prop_by_prop_smooth
oosPlots$basisCorrel_prop_by_prop_smooth
oosPlots$basisCorrel_prop_by_prop_smooth + labs(caption = '')
rm(list = ls())
setwd('c:/users/coch0/Documents/GitHub/TEfits/R/')
setwd('..')
library(devtools) ; document()
library(devtools) ; install()
library(TEfits)
library(lme4)
?time_basisFun_mem
,basisDens = 50
library(lme4)
## generate data
d <- data.frame(
subID = rep(c('A','B','C'),each=200)
, x = rbinom(600,1,.5)
, sinOffset = sin((1:600)/10) # one possible kind of change: oscillation without trend
, trialNum = rep(1:200, 3)
) ; d$y <- rnorm(600) + d$x + d$sinOffset
## fit a model with default basis widths
m1 <- time_basisFun_mem(
y ~ x + (x|subID)
,d
,groupingVarName = 'subID'
,timeVarName = 'trialNum'
)
## overall model summary:
summary(m1)
## extract the fitted timecourse (using `lme4` because this is an `lmer` model):
m1_fitted_timeCourse <- predict(m1,random.only=T
,re.form = as.formula(paste('~',gsub('x + (x | subID) + ','',as.character(formula(m1))[3],fixed=T)) ) )
plot(d$trialNum,m1_fitted_timeCourse)
## it doesn't look very good, because the default basis function width is fairly wide to prevent overfitting
## let's compare two models' out-of-sample likelihoods and choose the best
## The default, conservative, size (see m1)
m2 <- time_basisFun_mem(
y ~ x + (x|subID)
,d
,groupingVarName = 'subID'
,timeVarName = 'trialNum'
,basisDens = 50
,n_oos = 50
)
## A less-dense set of bases, every 20 trials
m3 <- time_basisFun_mem(
y ~ x + (x|subID)
,d
,groupingVarName = 'subID'
,timeVarName = 'trialNum'
,basisDens = 20
,n_oos = 50
)
## This takes noticeably longer to run
## approximate Cohen's D of the out-of-sample log-likelihoods for m3 over m2:
(mean(attr(m3,'delta_logLik_outOfSample')) - mean(attr(m2,'delta_logLik_outOfSample')) )/
sd(c(attr(m3,'delta_logLik_outOfSample'),attr(m2,'delta_logLik_outOfSample')))
## Clearly m3's out-of-sample predictiveness is better than m2's
## What about its fitted timecourse?
m3_fitted_timeCourse <- predict(m3,random.only=T
,re.form = as.formula(paste('~',gsub('x + (x | subID) + ','',as.character(formula(m3))[3],fixed=T)) ) )
plot(d$trialNum,m3_fitted_timeCourse)
rm(list = ls())
setwd('vignettes/')
setwd('..')
library(devtools) ; document()
library(devtools) ; install(build_vignettes = T)
library(devtools) ; install(build_vignettes = T)
rm(list = ls())
